{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DMLdXVdZjGac"
      },
      "outputs": [],
      "source": [
        "# Código para Descompactar o arquivo que será usado para o Tech Challenge 3 Fine-Tuning\n",
        "# Tratamento inicial desconsiderando dados nullos ou vazios para as colunas title e content\n",
        "# Salvar os dados em formato parquet para grandes bases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import json\n",
        "import gzip # Importar a biblioteca gzip\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA20NPVij7XM",
        "outputId": "d8b2230c-7973-4edf-8242-c859daec534e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Fiap/LF-Amazon-1.3M.raw.zip'"
      ],
      "metadata": {
        "id": "N6lFUpaWkRKB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lista os arquivos dentro do zip\n",
        "try:\n",
        "    with zipfile.ZipFile(file_path, 'r') as zf:\n",
        "        print(\"Arquivos dentro do ZIP:\")\n",
        "        for file_info in zf.infolist():\n",
        "            print(file_info.filename)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo ZIP '{file_path}' não foi encontrado.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Erro: O arquivo '{file_path}' não é um arquivo ZIP válido.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ahMdDkpKTG",
        "outputId": "92b79f71-0214-46c9-dd71-a2b6a15d0ae0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivos dentro do ZIP:\n",
            "LF-Amazon-1.3M/\n",
            "LF-Amazon-1.3M/lbl.json.gz\n",
            "LF-Amazon-1.3M/trn.json.gz\n",
            "LF-Amazon-1.3M/filter_labels_test.txt\n",
            "LF-Amazon-1.3M/tst.json.gz\n",
            "LF-Amazon-1.3M/filter_labels_train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ccecc2",
        "outputId": "7b2c4813-17ef-4f49-a96d-3955ef939480"
      },
      "source": [
        "# Descompactar o arquivo zip\n",
        "file_path = '/content/drive/MyDrive/Fiap/LF-Amazon-1.3M.raw.zip'\n",
        "extracted_data_trn = [] # Nova lista para armazenar os dados extraídos de trn.json.gz\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(file_path, 'r') as zf:\n",
        "        # Caminho para o arquivo .gz dentro do ZIP\n",
        "        gz_file_path_in_zip = 'LF-Amazon-1.3M/trn.json.gz'\n",
        "        with zf.open(gz_file_path_in_zip, 'r') as gz_file:\n",
        "            # Descompactar o arquivo .gz e ler linha por linha\n",
        "            # Usar gzip.open com 'rt' para ler como texto e iterar por linhas\n",
        "            with gzip.open(gz_file, 'rt', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    # Remover espaços em branco no início/fim da linha antes de tentar decodificar\n",
        "                    stripped_line = line.strip()\n",
        "                    if not stripped_line: # Pular linhas vazias\n",
        "                        continue\n",
        "                    try:\n",
        "                        # Tenta carregar cada linha como um objeto JSON individual\n",
        "                        data_item = json.loads(stripped_line)\n",
        "                        # Extrai as chaves desejadas, verificando se existem\n",
        "                        uid = data_item.get('uid')\n",
        "                        title = data_item.get('title')\n",
        "                        content = data_item.get('content')\n",
        "\n",
        "                        # Adiciona os dados extraídos a uma lista, se pelo menos uma chave existir\n",
        "                        # ou se a chave existe e não é None/vazia (dependendo da sua regra)\n",
        "                        # Aqui, adicionamos se pelo menos uma das chaves existe e não é None\n",
        "                        if uid is not None or title is not None or content is not None:\n",
        "                             # Adicionar os dados extraídos, garantindo que as chaves ausentes sejam None\n",
        "                             extracted_data_trn.append({\n",
        "                                 'uid': uid,\n",
        "                                 'title': title,\n",
        "                                 'content': content\n",
        "                             })\n",
        "\n",
        "                    except json.JSONDecodeError:\n",
        "                        # Imprimir a linha que causou o erro para depuração\n",
        "                        print(f\"Aviso: Não foi possível decodificar a linha JSON do arquivo '{gz_file_path_in_zip}': {stripped_line[:200]}...\") # Mostra apenas os primeiros 200 caracteres\n",
        "                        continue # Pula para a próxima linha se a decodificação falhar\n",
        "\n",
        "\n",
        "    print(f\"Processamento de '{gz_file_path_in_zip}' concluído. Total de itens extraídos: {len(extracted_data_trn)}\")\n",
        "    # Opcional: Imprimir uma amostra dos dados extraídos\n",
        "    # print(extracted_data_trn[:5])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo ZIP '{file_path}' não foi encontrado.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"Erro: O arquivo '{file_path}' não é um arquivo ZIP válido.\")\n",
        "except KeyError:\n",
        "    print(f\"Erro: O arquivo '{gz_file_path_in_zip}' não foi encontrado dentro do arquivo ZIP.\")\n",
        "except gzip.BadGzipFile:\n",
        "    print(f\"Erro: O arquivo '{gz_file_path_in_zip}' dentro do ZIP não é um arquivo gzip válido.\")\n",
        "# O json.JSONDecodeError específico para json.load() não ocorrerá mais com a leitura linha a linha\n",
        "# except json.JSONDecodeError:\n",
        "#     print(f\"Erro: Não foi possível decodificar o JSON do arquivo descompactado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro inesperado: {e}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processamento de 'LF-Amazon-1.3M/trn.json.gz' concluído. Total de itens extraídos: 2248619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Inicializar a sessão Spark (se não estiver inicializada)\n",
        "# É importante que o Spark tenha memória suficiente configurada, mas vamos tentar\n",
        "# ler diretamente de um arquivo para evitar o problema de memória no driver.\n",
        "spark = SparkSession.builder.appName(\"AmazonDataSpark\").getOrCreate()\n",
        "\n",
        "print(\"Sessão Spark inicializada.\")\n",
        "\n",
        "# Salvar a lista extracted_data_trn em um arquivo JSON Lines temporário\n",
        "temp_json_file = \"/tmp/extracted_data_trn.jsonl\"\n",
        "try:\n",
        "    with open(temp_json_file, 'w', encoding='utf-8') as f:\n",
        "        for item in extracted_data_trn:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    print(f\"Dados salvos temporariamente em '{temp_json_file}'.\")\n",
        "\n",
        "    # Ler o arquivo JSON Lines diretamente no Spark DataFrame\n",
        "    # Spark pode ler arquivos JSON Lines usando read.json()\n",
        "    df = spark.read.json(temp_json_file)\n",
        "\n",
        "    print(\"DataFrame Spark criado com sucesso a partir do arquivo!\")\n",
        "\n",
        "    # Opcional: Exibir o esquema do DataFrame Spark\n",
        "    df.printSchema()\n",
        "\n",
        "    # Opcional: Exibir as primeiras linhas do DataFrame Spark\n",
        "    df.show(5)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")\n",
        "finally:\n",
        "    # Limpar o arquivo temporário (opcional, mas boa prática)\n",
        "    if os.path.exists(temp_json_file):\n",
        "        os.remove(temp_json_file)\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMzEwU56o5ha",
        "outputId": "099f03f4-31e9-4fae-bebb-66e0ec83c95d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sessão Spark inicializada.\n",
            "Dados salvos temporariamente em '/tmp/extracted_data_trn.jsonl'.\n",
            "DataFrame Spark criado com sucesso a partir do arquivo!\n",
            "root\n",
            " |-- content: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- uid: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+----------+\n",
            "|             content|               title|       uid|\n",
            "+--------------------+--------------------+----------+\n",
            "|High quality 3 la...|Girls Ballet Tutu...|0000031909|\n",
            "|                    |Adult Ballet Tutu...|0000032034|\n",
            "|                    |The Way Things Wo...|0000913154|\n",
            "|Judith Kerr&#8217...|       Mog's Kittens|0001360000|\n",
            "|                    |Misty of Chincote...|0001381245|\n",
            "+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"amazon_data\")\n",
        "# Executar uma consulta SQL para filtrar registros com 'content' não nulo e não vazio\n",
        "df_filtered = spark.sql(\"\"\"\n",
        "        SELECT distinct title, content\n",
        "        FROM amazon_data\n",
        "        WHERE 1=1\n",
        "        and content IS NOT NULL  AND trim(content) != ''\n",
        "        and title IS NOT NULL AND trim(title) != ''\n",
        "        and len(title) > 3\n",
        "        and len(content) > 4\n",
        "    \"\"\")\n"
      ],
      "metadata": {
        "id": "cjdmc-Zcv2LO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.createOrReplaceTempView(\"df_filtered\")\n",
        "#Processamento de 'LF-Amazon-1.3M/trn.json.gz' concluído. Total de itens extraídos: 2248619\n",
        "df_filtered.count()\n",
        "#Total de registro após o distinct 1364345"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-PmMdDxcjRD",
        "outputId": "c0643fc1-e3f4-42e5-8d22-9a93eca1c18e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1364345"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o DataFrame df_filtered em formato Parquet no Google Drive\n",
        "output_path_parquet = '/content/drive/MyDrive/Fiap/amazon_data_parquet'\n",
        "\n",
        "# Spark pode escrever DataFrames em vários formatos, incluindo Parquet.\n",
        "# Usaremos o modo 'overwrite' para substituir o arquivo/diretório se já existir.\n",
        "try:\n",
        "    df_filtered.write.mode('overwrite').parquet(output_path_parquet)\n",
        "    print(f\"DataFrame df_filtered salvo com sucesso em '{output_path_parquet}' em formato Parquet.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro ao salvar o DataFrame em formato Parquet: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mETQ_wXRcjgq",
        "outputId": "91057a03-93c7-48f6-f037-0d70d214dd27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame df_filtered salvo com sucesso em '/content/drive/MyDrive/Fiap/amazon_data_parquet' em formato Parquet.\n"
          ]
        }
      ]
    }
  ]
}